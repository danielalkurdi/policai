name: Automated AI Policy Scraping

# This workflow automatically discovers and imports Australian AI policy content
# Runs every 6 hours and can also be triggered manually

on:
  schedule:
    # Run every 6 hours at minute 0
    - cron: '0 */6 * * *'

  # Allow manual trigger from GitHub Actions tab
  workflow_dispatch:
    inputs:
      force_all:
        description: 'Force run all scrapers (ignore schedules)'
        required: false
        type: boolean
        default: false

jobs:
  scrape-policies:
    name: Scrape AI Policy Content
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Full history for potential commit

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run scrapers
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          NEXT_PUBLIC_API_URL: ${{ secrets.NEXT_PUBLIC_API_URL || 'http://localhost:3002' }}
        run: |
          echo "Starting AI policy scraper..."
          npm run scrape

      - name: Upload scraper logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: |
            logs/
            data/scraper-state.json
          retention-days: 30

      - name: Check for new policies
        id: check_changes
        run: |
          git diff --exit-code public/data/sample-policies.json || echo "changes=true" >> $GITHUB_OUTPUT

      - name: Commit and push new policies (optional)
        if: steps.check_changes.outputs.changes == 'true'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add public/data/sample-policies.json
          git add public/data/pending-content.json
          git commit -m "chore: auto-update policies from scraper [skip ci]"
          git push
        # Note: This step is commented out by default for safety
        # Uncomment when you're confident in the scraper quality
        # env:
        #   GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Create summary
        if: always()
        run: |
          echo "## Scraper Run Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Run Time:** $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f "logs/scraper.log" ]; then
            echo "### Last 20 log lines:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -20 logs/scraper.log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Notify on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'ðŸš¨ Scraper failed - ' + new Date().toISOString(),
              body: 'The automated policy scraper encountered an error. Check the workflow logs for details.',
              labels: ['scraper', 'automated']
            })

# To enable this workflow:
# 1. Rename this file from scraper.yml.example to scraper.yml
# 2. Add ANTHROPIC_API_KEY to your repository secrets:
#    - Go to Settings â†’ Secrets and variables â†’ Actions
#    - Click "New repository secret"
#    - Name: ANTHROPIC_API_KEY
#    - Value: your-claude-api-key
# 3. (Optional) Add NEXT_PUBLIC_API_URL if your API is hosted separately
# 4. Push to main branch
